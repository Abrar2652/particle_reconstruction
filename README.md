<h1 align="center">
End-to-End Particle Reconstruction for the CMS Experiment </br>
</h1>

***

## Table of Contents
- [Installation](#installation)
- [Abstract](#abstract)
- [Introduction](#introduction)
- [Data Pre-processing](#data-pre-processing)
- [Training](#training)
  * [Pipelines](#pipelines)
  * [Models](#models)
- [Results](#results)
- [Notebooks and Docs](#notebooks-and-docs)
- [References](#references)
- [Acknowledgement](#acknowledgement)
- [Contributing and Reporting](#contributing-and-reporting)


## Installation

```shell
!git clone https://github.com/Abrar2652/particle_reconstruction.git
%mv particle_reconstruction Transformers
%cd Transformers
```

## Abstract
If the reader knows what they are dealing with, I would advise proceeding further.

In this project, we are working with multi-detector images corresponding to actual maps of low-level
energy deposits in the detector for various types of particle collision events in CMS(CERN).
In layman's terms, we use the data generated by the detectors in the CMS experiment to 
learn more about the particles and analyse them. The used in our case are Monte-Carlo simulated 
data. We are using the following data and constructing images(jet images) out of them, and using
Machine Learning to analyse(in our case Classify) them. Different types of detectors or sensors
produce different kinds of data or, in our case, jet images.


***
## Introduction
Previously there have been attempts to classify the images with ResNet-based architectures.
In our case we will use Transformers, namely Vision Transformers and various state-of-the-art
Transformer based architectures to achieve a somewhat higher score than the previous attempts.
To see if our hypothesis is indeed true and the ViT are performing better we have tested the models
across two different datasets namely the **Quark-Gluon** dataset and the **Boosted Top-Quark** dataset.
Since the two datasets were quite different from one another two different approaches were used in training
and deploying them. The exact process is decribed below.


## Data Pre-processing
ViTs are very sensitive to data pre-processing techniques. It has often been seen that even things like Image Interpolation Techniques, if not done properly, can adversely affect the performance of Transformers-based models. In our case, the data is directly sourced from CMS Open Data, and the outputs(pixel) can be arbitrarily large for a single detector(calorimeter) hence proper normalization techniques are employed.
We employ the following steps to ensure that the smaples or our datapoints are properly normalised and free from outliers:
* Zero suppression for any value under 10^-3.
* Channel Wise Z-score Normalisation across batches of size 4096.
* Channel Wise Max value clipping with clip value set equal to 500 times the standard deviation of the pixel values.
* Sample wise Min Max scaling.
Although the pre-processing is same for both the datasets but the input pipelines are vastly difference due to the training environment and computational challenges.

The following tutorials will help the reader to generate the processed data for training.
- Quark-Gluon(.png type dataset)
  * [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1Z9nVN_4cusHeqbfAXwEpyQIaA9aX39-c?usp=sharing)
- Boosted Top-Quark(TFRecord dataset)
  * [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1nW_0II1LglT0VsG6T7pfxw-Yj4zVsUcl?usp=sharing)



## References
* [End-to-End Jet Classification of Quarks and Gluons with the CMS Open Data](https://arxiv.org/abs/1902.08276)
* [End-to-End Jet Classification of Boosted Top Quarks with the CMS Open Data](https://arxiv.org/abs/2104.14659)
* [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929)
* [Swin Transformer: Hierarchical Vision Transformer using Shifted Windows](https://arxiv.org/abs/2103.14030)
* [CoAtNet: Marrying Convolution and Attention for All Data Sizes](https://arxiv.org/abs/2106.04803)
* [MaxViT: Multi-Axis Vision Transformer](https://arxiv.org/abs/2204.01697)
* [DaViT: Dual Attention Vision Transformers](https://arxiv.org/abs/2204.03645)
* [EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks](https://arxiv.org/abs/1905.11946)





